---
title: "999_functions"
author: "Jagadeesh Puvvula"
date: "2023-07-21"
output: pdf_document
---

#' Generated missing data frequency and percent
#' @param df input dataframe should be in wide format
#' @return returns a datatable with variable name, count and frequency in the global environment
#' export
```{r}
missing_data_summary <- function(df) {
  # Create a dataframe with the count of missing values for each variable
  missing_counts <- df |> 
    summarise_all(~ sum(is.na(.))) |> 
    gather(variable, missing_count)
  
  # Filter out variables with no missing values
  missing_counts <- missing_counts |> 
    filter(missing_count > 0)
  
  # Calculate the percent of missing values for each variable
  missing_percents <- df |> 
    summarise_all(~ mean(is.na(.))) |> 
    gather(variable, missing_percent)
  
  # Merge the count and percent dataframes
  missing_summary <- left_join(missing_counts, missing_percents, by = "variable")
  
  # Print the summary table
  missing_summary |> 
    mutate(missing_percent = scales::percent(missing_percent)) |> 
    arrange(desc(missing_count))
}
```

#' imputation using SuperLearner 
#' exclusive to the mixture analysis project
#' @param data takes a dataframe with missing information - assuming missing at random - unmeasured data
#' @param var_name user input's a variable of interest that has missing observations and that need to be imputed
#' @return retuns a complete dataframe withouth missing data and prints the MSE value for the prediction model
```{r}
impute_missing <- function(data, var_name) {
  
  # Impute missing values with median, except for the variable mentioned
  for (col in names(data)) {
    if (col != var_name && any(is.na(data[[col]]))) {
      data[[col]][is.na(data[[col]])] <- median(data[[col]], na.rm = TRUE)
    }
  }
  
  return(data)
}


# PREDICTING UNKNOWN 
generate_x <- function(sl.obj, pred_set, id_col, pred_var, rounding = 1){
  
  # predict variable using SuperLearner and impute
  imputed <- as.data.frame(predict.SuperLearner(sl.obj, pred_set[-c(id_col, which(names(pred_set) == pred_var))]))
  
  # create data frame with subject_id and predicted variable
  x <- cbind(subject_id = pred_set[, id_col], as.numeric(imputed$pred))
  
  # set the name of the predicted variable
  colnames(x)[2] <- pred_var
  
  # round the predicted variable to the specified number of digits
  x[[pred_var]] <- round(x[[pred_var]], digits = rounding)
  
  # return the resulting data frame
  return(x)
}

# replacing missing values 
replace_values <- function(x, original) {
  
  # Convert subject_id in x and original to factors
  x$subject_id <- factor(x$subject_id)
  original$subject_id <- factor(original$subject_id)
  
  # Extract the name of the variable to replace from x
  var_name <- names(x)[2]
  
  # Replace values in original with values from x
  original[[var_name]] <- ifelse(original$subject_id %in% x$subject_id, 
                                  x[[var_name]][match(original$subject_id, x$subject_id)], 
                                  original[[var_name]])
  
  # Return the updated original dataframe
  return(original)
}
```

# Variable selection using 4 algorithms
# group LASSO (grpreg and gglasso), sparse group lasso, and stability selection with error control
# This function will take a matrix of X (which represents exposures/covariates), Y (reperesent outcomes) 
# and a list group (to classify metric from matrix X to different groups)
# the output will be a list of lists that need to be compiled into a dataframe using cbind
```{r}
var_selec <- function(X, Y, group) {
  #load libraries
  pacman::p_load(gglasso, grpreg, sparsegl, Matrix, glmnet, stabs)
  results <- list()
  for (i in 1:ncol(Y)) {
    y <- Y[, i]
    # Group graphical Lasso
    set.seed=5678
    gr_cv <- cv.gglasso(X, y, group=group, loss="ls", pred.loss="L2",  nfolds=10)
    gr_min_beta <- coef(gr_cv, s = gr_cv$lambda.min)[-1]
  
    # Group Lasso
    grpp_cv <- cv.grpreg(X, y, group = group, penalty="grLasso",seed=5678,nfolds = 10)
    grpp_min_beta <- coef(grpp_cv, s = grpp_cv$lambda.min)[-1]
  
    #Sparse lasso
    sparse_cv<- cv.sparsegl(X, y, group = group, family = "gaussian", nfolds = 10)
    sparse_min_beta<- coef(sparse_cv, s= sparse_cv$lambda.min)[-1]


    #Stability selection with error control - input cross-validated lambda.min from cv-glmnet
    stab_lambda_min <- cv.glmnet(X, y, nfolds=10)$lambda.min
    stab_maxCoef <- stabsel(X, y, fitfun = glmnet.lasso_maxCoef, args.fitfun = list(lambda = stab_lambda_min), cutoff = 0.75, PFER = 1)
    stab_maxCoef_selec<- stab_maxCoef$max
  
    # Store results in list
    results[[paste0("outcome", i)]] <- as.data.frame(list(gr_lasso = gr_min_beta, 
                                                          grpp_lasso = grpp_min_beta,
                                                          sparse_lasso = sparse_min_beta,
                                                          stab_cv_glmnet = stab_maxCoef_selec))
  }
  # Return list of results
  return(results)
  
  # Unload the libraries after the function is complete - conflicts with tidyverse
  detach(package:gglasso)
  detach(package:grpreg)
  detach(package:sparsegl)
  detach(package:Matrix)
  detach(package:glmnet)
  detach(package:stabs)
}
```

# Linear regression for mixtures project 
# Associations between a single chemical and an outcome per iteration adjusting for covariates
# This function needs variable named sex and/or cohort to turn on the include_sex/include_cohort for stratified analysis
# Input formar - dataframe. User need to provide list of dependent, indipendent variables, covariates and a dataframe
# This function will produce a dataframe that will contain observations each of them is a linear regression result with a 
# combination between exposure and outcome variable
```{r}
lm_exp_mixtures <- function(dependent_vars, independent_vars, covariates, data, include_sex = TRUE, include_cohort = TRUE) {
  # create empty lists to store results
  dependent_list <- list()
  independent_list <- list()
  sex_level_list <- list()
  cohort_level_list <- list()
  coef_list <- list()
  p_value_list <- list()

  for (sex_level in c("all", "Female", "Male")) {
    if (include_sex) {
      if (sex_level == "all") {
        sex_data <- data
        sex_formula <- "+sex"
      } else {
        sex_data <- subset(data, sex == sex_level)
        sex_formula <- ""
      }
    } else {
      sex_data <- data
      sex_formula <- ""
    }

    for (cohort_level in c("all", "home", "mirec")) {
      if (include_cohort) {
        if (cohort_level == "all") {
          cohort_data <- sex_data
          cohort_formula <- "+cohort + city"
        } else if (cohort_level == "home") {
          cohort_data <- subset(sex_data, cohort == "1")
          cohort_formula <- ""
        } else if (cohort_level == "mirec") {
          cohort_data <- subset(sex_data, cohort == "2")
          cohort_formula <- "+city"
        } else {
          stop("Invalid cohort level")
        }
      } else {
        cohort_data <- sex_data
        cohort_formula <- ""
      }

      # loop through all combinations of dependent and independent variables
      for (i in 1:length(dependent_vars)) {
        for (j in 1:length(independent_vars)) {
          # run linear regression with covariates
          formula <- as.formula(paste(dependent_vars[i], "~", paste(independent_vars[j], "+", 
                                                                    paste(covariates, collapse = " + "), 
                                                                    sex_formula, cohort_formula)))
          model <- lm(formula, cohort_data)

          # store results in lists
          dependent_list[[length(dependent_list) + 1]] <- dependent_vars[i]
          independent_list[[length(independent_list) + 1]] <- independent_vars[j]
          sex_level_list[[length(sex_level_list) + 1]] <- sex_level
          cohort_level_list[[length(cohort_level_list) + 1]] <- cohort_level
          coef_list[[length(coef_list) + 1]] <- coef(model)[independent_vars[j]]
          p_value_list[[length(p_value_list) + 1]] <- summary(model)$coefficients[independent_vars[j], 4]
        }
      }
    }
  }

  # create dataframe with results
  results <- data.frame(
    dependent_variable = unlist(dependent_list),
    independent_variable = unlist(independent_list),
    sex_level = unlist(sex_level_list),
    cohort_level = unlist(cohort_level_list),
    coefficient = unlist(coef_list),
    p_value = unlist(p_value_list)
  )

  # return dataframe
  return(results)
}
```



#qgcomp_loop
```{r}
qgcomp_func <- function(outcomes, data, output_folder, chemicals, covariates,
                        include_sex = TRUE, include_cohort = TRUE, q, b, seed) {

  sex_levels <- if (include_sex) c("all", "Female", "Male") else "all"
  cohort_levels <- if (include_cohort) c("all", "home", "mirec") else "all"

  for (sex_level in sex_levels) {
    if (sex_level == "all") {
      sex_data <- data
      sex_formula <- "sex"
    } else {
      sex_data <- subset(data, sex == sex_level)
      sex_formula <- ""
    }

    for (cohort_level in cohort_levels) {
      if (cohort_level == "all") {
        cohort_data <- sex_data
        cohort_formula <- "+cohort + city"
        filename_prefix <- paste0(sex_level, "_all")
      } else if (cohort_level == "home") {
        cohort_data <- subset(sex_data, cohort == "1")
        cohort_formula <- ""
        filename_prefix <- paste0(sex_level, "_home")
      } else if (cohort_level == "mirec") {
        cohort_data <- subset(sex_data, cohort == "2")
        cohort_formula <- "+city"
        filename_prefix <- paste0(sex_level, "_mirec")
      } else {
        stop("Invalid cohort level")
      }

      for (outcome in outcomes) {
        formula <- as.formula(paste(outcome, "~",
                                    paste(chemicals, collapse = " + "), "+",
                                    sex_formula, "+",
                                    cohort_formula, "+",
                                    paste(covariates, collapse = " + ")))

        nb <- qgcomp.noboot(formula, expnms = mixture, data = cohort_data, family = gaussian(), q = q)
        boot <- qgcomp.boot(formula, expnms = mixture, data = cohort_data, family = gaussian(), q = q, B = b, seed = seed)
        save(nb, file = paste0(output_folder, "/", "nb_", filename_prefix, "_", outcome, ".rda"))
        save(boot, file = paste0(output_folder, "/", "boot_", filename_prefix, "_", outcome, ".rda"))
      }
    }
  }
}
```

#get qgcomp estimates
```{r}
get_gcomp_estimates<- function(results_folder) {
  # get the file names in the folder
  file_names <- list.files(results_folder)

  # create empty data frame to store the results
  results <- data.frame(file = character(),
                        estimate = numeric(),
                        lower_ci = numeric(),
                        upper_ci = numeric(),
                        stringsAsFactors = FALSE)

  #loops through all .rda files and extract estimates, CI & p-values
  for (file_name in file_names) {
    # load the object from the file
    load(file.path(results_folder, file_name))
    # extract the object name using grep or grepl
    object_name <- ls()[grep("^boot|^nb", ls())]
    # extract the required information using the summary function and the object name
    estimate <- get(object_name)$coef["psi1"]
    CI <- get(object_name)$ci
    lower_ci <- CI[1]
    upper_ci <- CI[2]
    p_value <- get(object_name)$pval[2]
    # split file name by "_" and remove ".rda"
    file_parts <- strsplit(gsub("\\.rda", "", file_name), "_")[[1]]
    # assign parts to variables
    part1 <- file_parts[1]
    part2 <- file_parts[2]
    part3 <- file_parts[3]
    part4 <- file_parts[5]
    # print out the values
    cat("File:", file_name, "\n")
    cat("  Estimate:", estimate, "\n")
    cat("  Lower CI:", lower_ci, "\n")
    cat("  Upper CI:", upper_ci, "\n")
    cat("  p-value:", p_value, "\n")
    cat("  Part 1:", part1, "\n")
    cat("  Part 2:", part2, "\n")
    cat("  Part 3:", part3, "\n")
    cat("  Part 4:", part4, "\n")
    # add the results to the data frame
    results <- rbind(results, data.frame(boot_strp = part1,
                                         gender = part2,
                                         cohort = part3,
                                         outcome = part4,
                                         estimate = estimate,
                                         lower_ci = lower_ci,
                                         upper_ci = upper_ci,
                                         p_value = p_value,
                                         stringsAsFactors = FALSE))
    # remove the object from the R environment to avoid conflicts with other objects (all objects saved as nb or boot)
    rm(list = object_name)
  }

  return(results)
}

```

#extract qgcomp weights
```{r}
extract_weights <- function(folder_location) {
  # get list of files that start with "nb"
  nb_files <- list.files(path = folder_location, pattern = "^nb.*\\.rda$", full.names = TRUE)

  # create an empty list to store results
  results_list <- list()

  # loop through each nb file
  for (file in nb_files) {
    # load data from file
    load(file)

    # extract neg and pos weights
    neg <- as.data.frame(nb$neg.weights) |>
      rename(weight = `nb$neg.weights`) |>
      rownames_to_column("chemical") |>
      mutate(direction = "neg")

    pos <- as.data.frame(nb$pos.weights) |>
      rename(weight = `nb$pos.weights`) |>
      rownames_to_column("chemical") |>
      mutate(direction = "pos")

    weights_df <- bind_rows(neg, pos)

    # extract file name without path and .rda extension using regular expression
    file_name <- sub(".*/(.*)\\.rda", "\\1", file)

    # Add "file_name" as a variable to weights_df
    weights_df$file_name <- file_name

    # Extract psi values and add them as rows
    if (is.list(nb$psi)) {
      psi1 <- nb$psi$psi1
    } else {
      psi1 <- NA
    }

    psi_df <- data.frame(chemical = "neg.psi", weight = nb$neg.psi, direction = "neg_psi", file_name = file_name) |>
      rbind(data.frame(chemical = "pos.psi", weight = nb$pos.psi, direction = "pos_psi", file_name = file_name)) |>
      rbind(data.frame(chemical = "psi1", weight = nb$psi, direction = "psi1", file_name = file_name))

    # Append psi_df to weights_df
    combined_df <- bind_rows(weights_df, psi_df)

    # Append combined_df to results_list
    results_list[[file_name]] <- combined_df
  }

  # Combine all results into a single dataframe
  results_df <- do.call(rbind, results_list)

  return(results_df)
}

```

#bkmr multiple imputation
```{r}
run_parallel_bkmr <- function(zdfmi, y_var, x_vars, z_vars, log2_vars, num_imputations, iter = 5000, workers = 8) {
    # Set up parallel processing
    future::plan(strategy = future::multisession, workers = workers)
    
    # Initialize list for futures
    ff <- list()
    
    # Generate random seeds
    ss = round(runif(num_imputations) * .Machine$integer.max)
    
    # Function to convert factors to numeric
    convert_factors_to_numeric <- function(df) {
        for(col in names(df)) {
            if(is.factor(df[[col]]) || is.character(df[[col]])) {
                # Convert to factor first if character
                if(is.character(df[[col]])) {
                    df[[col]] <- as.factor(df[[col]])
                }
                # Convert factor to numeric, preserving levels
                df[[col]] <- as.numeric(df[[col]])
            }
        }
        return(df)
    }
    
    # Function to apply log2 transformation
    apply_log2_transform <- function(df, log2_vars) {
        for(col in log2_vars) {
            if(col %in% names(df)) {
                df[[col]] <- log2(df[[col]]+0.000001)
            }
        }
        return(df)
    }
    
    # Run parallel BKMR for each imputation
    for (ii in 1:num_imputations) {
        ff[[ii]] <- future({
            # Get complete data for this imputation
            current_data <- mice::complete(zdfmi, ii)
            
            # Convert factors to numeric
            current_data <- convert_factors_to_numeric(current_data)
            
            # Apply log2 transformation to specified variables
            current_data <- apply_log2_transform(current_data, log2_vars)
            
            # Extract X, Y, and Z from the completed dataset
            y = current_data[[y_var]]
            X = as.matrix(current_data[, x_vars])
            Z = as.matrix(current_data[, z_vars])
            
            # Run BKMR
            bkmr::kmbayes(y = y, 
                         X = X, 
                         Z = Z, 
                         iter = iter,
                         groups = c(rep(1,times=3), rep(2,times=6), rep(3, times=3),
                                  rep(4,times=4), rep(5,times=2), rep(6,times=2),
                                  rep(7,times=6), rep(8,times=3)),
                         verbose=TRUE, varsel=TRUE)
        }, seed = ss[ii])
    }
    
    # Collect results
    parallelfits <- future::value(ff)
    
    # Set class for parallel fits
    class(parallelfits) <- c("bkmrfit.list", "list")
    
    # Calculate parallel estimates across chains, 50% burnin
    ors_parallel = bkmrhat::OverallRiskSummaries_parallel(parallelfits)
    
    # Pooling sample to get summary estimate, 50% burnin
    pooledfit = kmbayes_combine(parallelfits)
    ors_pooled = bkmr::OverallRiskSummaries(pooledfit)
    
    # Calculate Rubin's rule estimator
    Vb = tapply(ors_parallel$est, ors_parallel$quantile, var)  # "between" variance
    Vw = tapply(ors_parallel$sd^2, ors_parallel$quantile, mean)  # "within" variance
    mn = tapply(ors_parallel$est, ors_parallel$quantile, mean)  # point estimate
    Vr = Vw + Vb*(1+1/num_imputations)
    quant = tapply(ors_parallel$quantile, ors_parallel$quantile, mean)
    
    # Create pooled estimates dataframe using Rubin's rules
    rubins_estimates <- data.frame(
        quantile = quant,
        est = mn,
        sd = sqrt(Vr)
    )
    
    # Return results
    return(list(
        parallel_fits = parallelfits,
        ors_parallel = ors_parallel,
        pooled_fit = pooledfit,
        ors_pooled = ors_pooled,
        rubins_estimates = rubins_estimates
    ))
}

```

